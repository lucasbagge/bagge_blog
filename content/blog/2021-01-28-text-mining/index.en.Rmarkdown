---
title: "Text mining"
author: "R package build"
date: '2021-01-28'
slug: text-mining
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-01-28T22:36:54+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = FALSE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(scales)
library(tidyverse)
library(rsample)
library(tidymodels)
library(textrecipes)
library(ggplot2)
theme_set(theme_minimal())
```

## Introduction

In this new series of post I will explain and make  Deel learning models to 
predict outcomes from predictors in text data. Thease models are different then
orther supervised learnings algoritmes such as *regularized lenear models*,
*support vector machines* and *naive bayes* because they are deep meaning they
use multiple layers to learn how to map from input features to output outcomes. 
Where the former use a shallow (single) mapping.

In this first post on Deep learning we wil explore a **densely connected neural 
network**. This type of model is not the one that is gonna review the best
performance on text data, but it is a great *null* model to start learning
about deep learning models. I will go though the different components in 
a deep learning model and what software to use.

The deep learning models is gonna be compared to a Naive Bayes model and look
which of those two get the best result.

For the analysis I am gonna use the data on 
[kaggle](https://www.kaggle.com/nltkdata/movie-review?select=movie_review.csv)
that is part of a competition I am participated in.

## Neural network

A neural network idea is based on the human brain where we the brain receive
an input from a sensor and goes though different layers before being
executed og end up in the brain. 

The input which is the data can be stored in different dimensions. They 
are called **tensors** and is a key concept and that is way Google´s `TensorFlow`
was named efter them.

So a tensor is basically data that can be in different dimensions and it is
defined by three key attributes:

- Number of axes (rank) - where axes is a *dimension*.
- Shape - This is an integer vector that describes how many dimensions the tensor
  has. In R you can see the dimensions with `dim()`.
- Data types - this is the type of dta contained in the tensor and should be
  a `integer` or `double`.
  
The math and calculations behind tensors relies a lot on **Linear Algebra** which
I will make another post for explaining.

The last step in this short introduction int the method in getting the right 
output. Here there is different optimization methods but one that is
used a lot is **gradient descent**:

![Gradient descent](gd.png)

So the picture explain the essens of what the optimization methods does. You
start of and goes to the final point but you eyees a blinded so under the
way to the final point you have to optimize the path.

### Anatomy of a neural network

So in a neural network we have the following objects:

- Layers tha are combined into a model/network.
- The input data and corresponding target.
- A loss function which defines the feedback signal used for learning.
- The optimizer which determines how the learning are proceeded.

There are diffent types of layers and a simple layer is one that is stored in
2D tensors of shape and they are processed by **densely connected layers**.

### Introduction to Keras

For this post I will use `kera` which is a deep learning framework to train
your model. It is a very used framework and is he second most used based 
on the following plot.

![Interest in deep learning framework over time](deepframework.png)

Keras is high level meaning it dosen´t provide fx tensor manipulation operation.
Here it realies on backend engine and you have the chosse between TensorFlow, 
Theano and Microsoft Cognitive Toolkit. A pro for using TensorFlow is that 
Keras is able to run on both CPUs and GPUs.

Know that we now a little about neural network and the packages kera we can
start modelling on movie reviews. 

## Data

The data is from kaggle which is a popular site for competing in building
models for institution or firms that has a problem and want it solved with
a model.

For this paticular task we har given movie reviews and we want to predict from
the text if it is positiv or negativ. 

Let us get an overview of the data

```{r}
moviereview <- read_csv("movie_review.csv")
  
moviereview %>% skimr::skim()
```

From the data it is of course the `tag` (it gives us if the review was positive
or negative) and th `text` (it gives a comment on the movie). So I need to
use the two variable to see if I can use the information in the text to predict
the tag. 

To get a little familiar with the data I will look at the distribution of
characters.

```{r}
moviereview %>%
  ggplot(aes(nchar(text))) +
  geom_histogram(binwidth = 1, alpha = 0.8) +
  labs(
    x = "Number of characters per text",
    y = "Number of text"
  )
```

The plot shows that the distribution is left-skewed and most reviewer docent
write that much. 

There is not much to say regarding the data because my focus is on building
the models and introduced deep learning and how it can be applied in R.

For making the models I will use a Naive Bayes model and a dense deep learning
model to classify the reviews and see which models performce best. 


## A Naive Bayes classification model.

For the first moel we are gonna build a binary model to predict the tag. Let us
look at the first reviews

```{r}
moviereview %$% text %>% head() 
```

As many other text data it is messy and it should be corrected. In
my preprocessinng step I will clean the data up.

For every machine learning project I will create a traning and testing set.

```{r}
df <- moviereview %>% mutate(tag = as_factor(ifelse(tag == "pos", 1, 0)))
moviereview_split <- df %>% initial_split(strata = tag)

train <- training(moviereview_split)  
test <- testing(moviereview_split)
```

For building the Naive Bayes model I am going to rely on `tidymoels` which
give a greate framework in building a model and apply it to the traning data.

First let us create the recipe which is a preprocessing step where we
tokenize the text column, remove stopwords, keep the max 150 frequent tokens,
so we dont get to many variable in the model. 

```{r}
complaints_rec <- recipe(tag ~ text, data = train) %>%
  step_tokenize(text) %>%
  textrecipes::step_stopwords(text) %>%
  step_tokenfilter(text, max_tokens = 500) %>%
  step_tfidf(text)
```

Now that we have a full specification of the preprocessing recip, we can `prep()`
this recipe to estimate all the parameter for each step using the traning data.

```{r}
complaint_prep <- prep(complaints_rec)
```

For most modelling takss you will not need to prep your recipe directly; instead
you can build up a `workflow()` to bundle together your modeling components.

```{r}
complaint_wf <- workflow() %>%
  add_recipe(complaints_rec)
```

The Naive Bayes moel is available in the package `discrim` and has a main advantages 
in its capacity to handle large number of features.

```{r}
library(discrim)
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")
```

Now we need to fit the model and for this we just have to add the Bayes model to
our workflow an then fit it to the traning data. 

```{r}
nb_fit <- complaint_wf %>%
  add_model(nb_spec) %>%
  fit(data = train)
```

As a additional step before going to test the model on the testing set we
will resample to evalutate our model. I am going to create 10 fold cross validation
set for seing the models performance estimates.

```{r}
set.seed(234)
complaints_folds <- vfold_cv(train)
```

Here we have 90% of the traning data in each fold and the other 10% is hel out 
for evaluation. 

```{r}
nb_wf <- workflow() %>%
  add_recipe(complaints_rec) %>%
  add_model(nb_spec)
```

Now we gonna fit it on the resampled fold.

```{r}
nb_rs <- fit_resamples(
  nb_wf,
  complaints_folds,
  control = control_resamples(save_pred = TRUE)
)
```

We can extract the relevant information using `collect_metrices` and 
`collect_predictions`.

```{r}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)
```

```{r}
nb_rs_metrics
```

For these resamples the average accuracy is 52 %.

We can also plot **receiver operator curve** that shows sensitivity at different
thresholds. It demonstrates how well a classification model can distinguish 
between classes. 

```{r}
nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = tag, .pred_0) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "Receiver operator curve for Reviewer",
    subtitle = "Each resample fold is shown in a different color"
  )
```

The model is not great at all so let us see if a deep learning model does 
a better job. 

## Deep learning

```{r}
df <- moviereview %>% mutate(tag = ifelse(tag == "pos", 1, 0))
moviereview_split <- moviereview %>%
  mutate(tag = as.numeric(tag)) %>% 
  initial_split(strata = tag)

train <- training(moviereview_split)  
test <- testing(moviereview_split)
```

```{r}
library(textrecipes)

max_words <- 2000
max_length <- 50

movie_rec <- recipe(~ text, data = train) %>%
  step_tokenize(text) %>%
  step_tokenfilter(text, max_tokens = max_words) %>%
  step_sequence_onehot(text, sequence_length = max_length)
```


```{r}
movie_prep <- prep(movie_rec)
mov_train <- bake(movie_prep, new_data = NULL, composition = "matrix")
```

```{r}
library(keras)

dense_model <- keras_model_sequential() %>% 
  layer_embedding(
    input_dim = max_words + 1,
    output_dim = 12,
    input_length = max_length
  ) %>% 
  layer_flatten() %>% 
   layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
dense_model
```

```{r}
dense_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{r}
dense_history <- dense_model %>%
  fit(
    x = mov_train,
    y = train$tag,
    batch_size = 512,
    epochs = 20,
    validation_split = 0.25,
    verbose = FALSE
  )
```


```{r}
plot(dense_history)
```

```{r}
set.seed(234)
mov_val <- validation_split(train, strata = tag)
```

```{r}
mov_analysis <- bake(movie_prep,
  new_data = analysis(mov_val$splits[[1]]),
  composition = "matrix"
)
dim(mov_analysis)
```

```{r}
mov_assess <- bake(movie_prep,
  new_data = assessment(mov_val$splits[[1]]),
  composition = "matrix"
)
dim(mov_assess)
```

```{r}
state_analysis <- analysis(mov_val$splits[[1]]) %>% pull(tag)
state_assess <- assessment(mov_val$splits[[1]]) %>% pull(tag)
```

```{r}
dense_model <- keras_model_sequential() %>%
  layer_embedding(
    input_dim = max_words + 1,
    output_dim = 12,
    input_length = max_length
  ) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

dense_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{r}
val_history <- dense_model %>%
  fit(
    x = mov_analysis,
    y = state_analysis,
    batch_size = 512,
    epochs = 10,
    validation_data = list(mov_assess, state_assess),
    verbose = FALSE
  )

val_history
```

```{r}
plot(val_history)
```


```{r}
library(dplyr)

keras_predict <- function(model, baked_data, response) {
  predictions <- predict(model, baked_data)[, 1]

  tibble(
    .pred_1 = predictions,
    .pred_class = if_else(.pred_1 < 0.5, 0, 1),
    state = response
  ) %>%
    mutate(across(
      c(state, .pred_class), ## create factors
      ~ factor(.x, levels = c(1, 0))
    )) ## with matching levels
}
```

```{r}
val_res <- keras_predict(dense_model, mov_assess, state_assess)
val_res
```

```{r}
metrics(val_res, state, .pred_class)
```

```{r}
val_res %>%
  conf_mat(state, .pred_class) %>%
  autoplot(type = "heatmap")
```