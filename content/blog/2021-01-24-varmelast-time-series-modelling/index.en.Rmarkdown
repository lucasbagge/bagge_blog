---
title: 'Varmelast'
author: Lucas Bagge
date: '2021-01-24'
slug: varmelast
categories: 
- Time Series
tags: 
- Xgboost
- machine learning
- prophet
subtitle: 
summary: 'Learn how to use ´timetk´ to build time series models.'
authors: [Lucas Bagge]
lastmod: '2021-01-24T22:50:11+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(highcharter)
library(httr)
library(dplyr)
library(magrittr)
library(tidyr)
library(janitor)
library(lubridate)
library(jsonlite)
library(purrr)
library(sweep)      
library(forecast)   
library(tidyquant)  
library(timetk)     
library(ggplot2)
library(tidymodels)
library(modeltime)
library(timetk)   
library(lubridate)
library(tidyverse)
library(widgetframe)
library(happyorsad)
library(tm)
library(jtools)
library(rvest)
library(tidyverse)
library(tidytext)
library(topicmodels)
library(kableExtra)
library(pdp)
library(patchwork)
library(widyr)
library(ggraph)
library(influential)
```

```{r, include=FALSE}
my_theme <- function() {
  theme_apa(legend.pos   = "none") +
    theme(panel.background = element_rect(fill = "gray96", colour = "gray96"),
          plot.background  = element_rect(fill = "gray96", colour = "gray96"),
          plot.margin      = margin(1, 1, 1, 1, "cm"),
          panel.border     = element_blank(),        # facet border
          strip.background = element_blank() )        # facet title background
}
```


# Introduction

The energy use of the different power plant around Copenhagen is an important
indicator of where there can be a shorted of of sources.

The site [varmelast](https://www.varmelast.dk/) offers a view of how the different
plans use the energy sources every hour.

In this post I am gonna build a function to extract the data on the site and
use `timetk` to build different models to forecast the time series.


## Data

We need to get data from [varmelast](https://www.varmelast.dk/) which we do with 
he following function. Unfortunately there is no download bottom so we need to
extract the data by ourselve with the`httr` package.

To make it more generic I am create a function `varme_last` that extract the data for us.
Here I specify the user can decide in what period the data is gonna be extracted.


```{r}
varme_last <- function(from = "",  to = "") {
  resp <- GET(
    paste0('https://www.varmelast.dk/api/v1/heatdata/historical?from=',
           from,'&to=',to,'&intervalMinutes=60&contextSite=varmelast_dk')
  )
  content <- fromJSON(httr::content(resp, 'text'))
  
  date <- content$times$timestamp
  
  df_list <- content$times$values
  
  date_tibble <- map_df(date, as_tibble)
  
  df_tibble <- map_df(df_list, as_tibble)
  
  df_wide <-
    df_tibble %>%
    select(-valueError) %>%
    pivot_wider(names_from =  key,
                values_from = value) %>%
    unnest()
  
  df <-
    df_wide %>% cbind(date_tibble) %>%
    mutate(date = ymd_hms(value),
           across(where(is.numeric), round, 2)) %>% 
    janitor::clean_names() %>%
    select(-value) %>%
    rename(
      Affaldsenergianlæg = be_vl_affald_ef,
      Kraftvarmeanlæg = be_vl_kraftv_ef,
      'Spidslast gas' = be_vl_spids_gas_ef,
      'Spidslast olie' = be_vl_spids_olie_ef,
      'Spidslast træpiller' = be_vl_bio_ef,
      'CO2 - Udledning' = be_vl_total_fak,
      'Lokal produktion' = local
    )
  df
}
```

With this function I can extract the data from all of the energy sources and
go as far back in time as possible. The site is relative new so we have to
see how far we can go back. 

As a start we can see if we can get data from *2020-01-01*.

```{r}
data <- varme_last(from = "2020-01-01", to = today())
```

```{r}
data %>% 
  dplyr::glimpse()
```
From the above view from `glimse` we have **2246** observation ans **17**
features.

```{r}
data %>% 
  summarise(date_min = min(date),
            date_max = max(date)) %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling()
```

So the earlist date is *2020-10-15*. SO the site is new pretty new.

Let us see if there is an na values in some of the features.

```{r}
data %>% summarise_all(funs(sum(is.na(.)))) %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling()
```

There is no na values. So it seems that there is a automatic  transition
between the power plants going on so the data is pretty consistent. 

Lets us have a general view of the data

```{r}
data %>%
  pivot_longer(cols = -c(date),
               names_to = "metric",
               values_to = "value") %>%
  ggplot(aes(x = date, y = value, colour = metric)) +
  geom_line() +
  my_theme() +
  labs(
    title = "Energy use of Copenhagen Power Plants",
    subtitle = "The data goes from 2020-10-15 to 2021-01-18",
    y = "MJ/s",
    x = "")
```

As can be seem from the `highcharter` plot there is a lot of sources that
could be interesting to model on. I am gonna chosse `Kraftvarmeanlæg` to
work with and build several models on.

## Model Data

I am gonna create the data that I will build my models around. Here I am gonna use
`date` and `Kraftvarmeanlæg`.

```{r}
df <- data %>%
  select(date, Kraftvarmeanlæg,
         Affaldsenergianlæg, dap_veks_forbrug_eff
         ) %>%
  rename(value = Kraftvarmeanlæg) %>%
  as_tibble()
```


## Traning and Testing data

As in every model building I am gonna split the data in a traning and testing data.
For building time series models the new packages `timetk` is a great additonal
tool for building such models and it comes with great ways of splitting
the data.

```{r}
split <- df %>%
  time_series_split(assess = "5 days", cumulative = TRUE)
```

Lets us get a view of the traning and testing data.

```{r}
split %>%
  tk_time_series_cv_plan() %>%
  plot_time_series_cv_plan(date, value, .interactive = FALSE)
```


From the plot we can see there is a general growing tendency in the data but
there is also a sharply decline in the periods. It would be ideal to have data
for a years so we could se if there was a clear seasonal effects.

From the split we can use `rsample` functions to build the traning and testing data.

```{r}
train <- training(split)
test <- testing(split)
```


# Modelling

From the traning data we a gonna build multiple models to try to forecast
Kraftvarmeanlæg.

This articles is meant as an introduction for time series so the math and theoretical
behinde each algoritmes is not something I will put a big foucs on. Additional
post will go into deep with each models and the math behinde them.

## Arima

```{r}
model_fit_arima <- arima_reg() %>%
  set_engine("auto_arima") %>%
  fit(value ~ date,
        train)
```

## Prophet

A prophet model is maybe most fomous of being a forecasting model developed by Facebook.

```{r}
model_fit_prophet <-
  prophet_reg(seasonality_daily = TRUE) %>%
  set_engine("prophet") %>%
  fit(value ~ date, train)
```

## ML models

They are more complex then the automated models. This means wee need to add
a workflow (also called pipeline). In the tidymodels framwork we have the
folowwing process:

- Create preprocessing recipe
- Create model specifications
- Use workflow (wf) to combine model spec and preprocessing and fit model

### Preprocessing Recipe

I want to create a preprocessing recipe with `recipe` and use some steps in
creating new features that is gonna be used in the model. These include
time series signature and fourier series.

```{r}
recipe_spec <- recipe(value ~ date, train) %>%
  step_timeseries_signature(date) %>%
  step_rm(contains("am.pm"), contains("hour"), contains("minute"),
          contains("second"), contains("xts")) %>%
  step_fourier(date, period =  365 / 12, K = 2) %>%
  step_dummy(all_nominal())

recipe_spec %>% prep() %>% juice()
```

WIth this recipe created we can use this as one of the ingretigens in a Machine
Learning pipeline.

### Elastic Net

Making a **Elastic Net** model is very easy to do here we just need to set the
spec to use `linear_reg()` and `set_engine("glmnet")`.

```{r}
model_spec_glmnet <- linear_reg(penalty = 0.1, mixture = 0.5) %>%
  set_engine("glmnet")
```

Notice here we have not fitted the model yet as we did in the firsts model. It
is because we are gonna fit the model in our workflow:

- Start with a workflow.
- Add a model spec.
- Add preprocessing.
-- Note here that we remove the date column because ML algorithms dont know
how to deal with date features.
- Fit the workflow.

```{r}
workflow_fit_glmnet <- workflow() %>%
  add_model(model_spec_glmnet) %>%
  add_recipe(recipe_spec %>%  step_rm(date)) %>%
  fit(train)
```


## New Hybrid models

As a new model I will showcase a **hybrid models** (a combination between
`arima_boost()` and `prophet_boost()`) that combine the two automated algorithms
with ML.

### Prophet Boost

The **Prophet Boost algorithm** combines Prophet with XGBoost to get the best
of the two. The algoritme works as follow:

1) First modeling the univariate series using Prophet
2) Using regressors supplied via the preprocessing recipe and regressing the
Prophet Residual with the XGBoost model

As with the other ML models we set it up in our workflow.

```{r}
model_spec_prophet_boost <- prophet_boost(seasonality_daily  = TRUE) %>%
  set_engine("prophet_xgboost")

workflow_fit_prophet_boost <- workflow() %>%
  add_model(model_spec_prophet_boost) %>%
  add_recipe(recipe_spec) %>%
  fit(train)
```


# The modeltime Workflow

With `modeltime` workflow we can speed up the model evaluation and it is very
useful know we have several time series models. In the next path I will analyze
them and forecast the future with the modeltime workflow.

## Modeltime table

The function `modeltime_table()` organizes the models with IDs and creates
generic descriptions to help us keep track of our models.

```{r}
model_table <- modeltime_table(
  model_fit_arima,
  model_fit_prophet,
  workflow_fit_glmnet,
  workflow_fit_prophet_boost
)
```

## Calibration

**Model Calibration** is used to quantify error and estimate confidence interval.
Here we gonna use calibration on our testing set with `modeltime_calibrate()`.
When using the function we create two new colums .type and .calibration_data
where the most important column is the former. This includes the actual values,
fitted vaues and residuals for the testing set.

```{r}
calibration_table <- model_table %>%
  modeltime_calibrate(test)
```

## Forecast (test set)

With the calibrated data we can visualize the testing prediction also called
forecast.

- Use `modeltime_forecast()` to generate the forecast data for the tesring set
  as a tiblle.
- Use `plot_modeltime_forecast()` to visualize the results.


```{r}
calibration_table %>%
  modeltime_forecast(actual_data = df) %>%
  plot_modeltime_forecast(.interactive = FALSE, .legend_show = TRUE)
```

## Accuracy (test set)

Next we can calculate the testing accuracy to compare the models.

- Use `modeltime_accuracy()` to generate the testing set metric as a tibble
- Use `table_modeltime_accuracy()` to generate a table.

```{r}
calibration_table %>%
  modeltime_accuracy() %>%
  knitr::kable() %>% 
  kableExtra::kable_styling()
```

## Analyse result


From the accuracy measures we can see that the est model is GLMNET.

## Refit

Refitting is a best practice before forecasting the future.

- `modeltime_refit()` here we gonna re train the model on the full data.
- `modeltime_forecast()` here we gonna forcecast on the date feature where
we we use the argument h to set the months or years.

```{r, warning=FALSE}
calibration_table %>%
  modeltime_refit(df) %>%
  modeltime_forecast(h = "1 month", actual_data = df) %>%
  plot_modeltime_forecast(.interactive = FALSE)
```

Here we can see that the GLMNET seem to  be a little bit off compared to the other
models.
