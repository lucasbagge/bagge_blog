---
title: "Text mining"
author: "R package build"
date: '2021-01-28'
slug: text-mining
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-01-28T22:36:54+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(scales)
library(tidyverse)
library(rsample)
library(tidymodels)
library(textrecipes)
library(ggplot2)
theme_set(theme_minimal())
```

## Introduction

In this new series of post I will explain and make  Deel learning models to 
predict outcomes from predictors in text data. Thease models are different then
orther supervised learnings algoritmes such as *regularized lenear models*,
*support vector machines* and *naive bayes* because they are deep meaning they
use multiple layers to learn how to map from input features to output outcomes. 
Where the former use a shallow (single) mapping.

In this first post on Deep learning we wil explore a **densely connected neural 
network**. This type of model is not the one that is gonna review the best
performance on text data, but it is a great *null* model to start learning
about deep learning models. I will go though the different components in 
a deep learning model and what software to use.

The deep learning models is gonna be compared to a Naive Bayes model and look
which of those two get the best result.

For the analysis I am gonna use the data on 
[kaggle](https://www.kaggle.com/nltkdata/movie-review?select=movie_review.csv)
that is part of a competition I am participated in.

## Neural network

A neural network idea is based on the human brain where we the brain receive
an input from a sensor and goes though different layers before being
executed og end up in the brain. 

The input which is the data can be stored in different dimensions. They 
are called **tensors** and is a key concept and that is way Google´s `TensorFlow`
was named efter them.

So a tensor is basically data that can be in different dimensions and it is
defined by three key attributes:

- Number of axes (rank) - where axes is a *dimension*.
- Shape - This is an integer vector that describes how many dimensions the tensor
  has. In R you can see the dimensions with `dim()`.
- Data types - this is the type of dta contained in the tensor and should be
  a `integer` or `double`.
  
The math and calculations behind tensors relies a lot on **Linear Algebra** which
I will make another post for explaining.

The last step in this short introduction int the method in getting the right 
output. Here there is different optimization methods but one that is
used a lot is **gradient descent**:

![Gradient descent](gd.png)

So the picture explain the essens of what the optimization methods does. You
start of and goes to the final point but you eyees a blinded so under the
way to the final point you have to optimize the path.

### Anatomy of a neural network

So in a neural network we have the following objects:

- Layers tha are combined into a model/network.
- The input data and corresponding target.
- A loss function which defines the feedback signal used for learning.
- The optimizer which determines how the learning are proceeded.

There are diffent types of layers and a simple layer is one that is stored in
2D tensors of shape and they are processed by **densely connected layers**.

### Introduction to Keras

For this post I will use `kera` which is a deep learning framework to train
your model. It is a very used framework and is he second most used based 
on the following plot.

![Interest in deep learning framework over time](deepframework.png)

Keras is high level meaning it dosen´t provide fx tensor manipulation operation.
Here it realies on backend engine and you have the chosse between TensorFlow, 
Theano and Microsoft Cognitive Toolkit. A pro for using TensorFlow is that 
Keras is able to run on both CPUs and GPUs.

Know that we now a little about neural network and the packages kera we can
start modelling on movie reviews. 

## Data

The data is from kaggle which is a popular site for competing in building
models for institution or firms that has a problem and want it solved with
a model.

For this paticular task we har given movie reviews and we want to predict from
the text if it is positiv or negativ. 

Let us get an overview of the data

```{r}
moviereview <- read_csv("movie_review.csv")
  
moviereview %>% skimr::skim()
```

From the data it is of course the `tag` (it gives us if the review was positive
or negative) and th `text` (it gives a comment on the movie). So I need to
use the two variable to see if I can use the information in the text to predict
the tag. 

To get a little familiar with the data I will look at the distribution of
characters.

```{r}
moviereview %>%
  ggplot(aes(nchar(text))) +
  geom_histogram(binwidth = 1, alpha = 0.8) +
  labs(
    x = "Number of characters per text",
    y = "Number of text"
  )
```

The plot shows that the distribution is left-skewed and most reviewer docent
write that much. 

There is not much to say regarding the data because my focus is on building
the models and introduced deep learning and how it can be applied in R.

For making the models I will use a Naive Bayes model and a dense deep learning
model to classify the reviews and see which models performce best. 


## A Naive Bayes classification model.

For the first moel we are gonna build a binary model to predict the tag. Let us
look at the first reviews

```{r}
moviereview %$% text %>% head() 
```

As many other text data it is messy and it should be corrected. In
my preprocessinng step I will clean the data up.

For every machine learning project I will create a traning and testing set.

```{r}
df <- moviereview %>% mutate(tag = as_factor(ifelse(tag == "pos", 1, 0)))
moviereview_split <- df %>% initial_split(strata = tag)

train <- training(moviereview_split)  
test <- testing(moviereview_split)
```

For building the Naive Bayes model I am going to rely on `tidymoels` which
give a greate framework in building a model and apply it to the traning data.

First let us create the recipe which is a preprocessing step where we
tokenize the text column, remove stopwords, keep the max 150 frequent tokens,
so we dont get to many variable in the model. 

```{r}
complaints_rec <- recipe(tag ~ text, data = train) %>%
  step_tokenize(text) %>%
  textrecipes::step_stopwords(text) %>%
  step_tokenfilter(text, max_tokens = 500) %>%
  step_tfidf(text)
```

Now that we have a full specification of the preprocessing recip, we can `prep()`
this recipe to estimate all the parameter for each step using the traning data.

```{r}
complaint_prep <- prep(complaints_rec)
```

For most modelling takss you will not need to prep your recipe directly; instead
you can build up a `workflow()` to bundle together your modeling components.

```{r}
complaint_wf <- workflow() %>%
  add_recipe(complaints_rec)
```

The Naive Bayes moel is available in the package `discrim` and has a main advantages 
in its capacity to handle large number of features.

```{r}
library(discrim)
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")
```

Now we need to fit the model and for this we just have to add the Bayes model to
our workflow an then fit it to the traning data. 

```{r}
nb_fit <- complaint_wf %>%
  add_model(nb_spec) %>%
  fit(data = train)
```

As a additional step before going to test the model on the testing set we
will resample to evalutate our model. I am going to create 10 fold cross validation
set for seing the models performance estimates.

```{r}
set.seed(234)
complaints_folds <- vfold_cv(train)
```

Here we have 90% of the traning data in each fold and the other 10% is hel out 
for evaluation. 

```{r}
nb_wf <- workflow() %>%
  add_recipe(complaints_rec) %>%
  add_model(nb_spec)
```

Now we gonna fit it on the resampled fold.

```{r}
nb_rs <- fit_resamples(
  nb_wf,
  complaints_folds,
  control = control_resamples(save_pred = TRUE)
)
```

We can extract the relevant information using `collect_metrices` and 
`collect_predictions`.

```{r}
nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)
```

```{r}
nb_rs_metrics
```

For these resamples the average accuracy is 52 %.

We can also plot **receiver operator curve** that shows sensitivity at different
thresholds. It demonstrates how well a classification model can distinguish 
between classes. 

```{r}
nb_rs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = tag, .pred_0) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "Receiver operator curve for Reviewer",
    subtitle = "Each resample fold is shown in a different color"
  )
```

The model is not great at all so let us see if a deep learning model does 
a better job. 

## Deep learning

One of the differnces using a deep learning model is the preprocessing.
Here we need to choose the length of the text and if the text is
longer then a certain length it will be truncated. The length is 
a hyperparameter and one need to make sure they don´t 

- overshoot so the text is given as zero.
- undershoot so we cut of a lot of text. 

We can count the word in the and select the right value.


```{r}
df <- moviereview %>%
  filter(nchar(text) >= 10) %>% 
  mutate(tag = ifelse(tag == "pos", 1, 0))
moviereview_split <- df %>%
  mutate(tag = as.numeric(tag)) %>% 
  initial_split(strata = tag)

train <- training(moviereview_split)  
test <- testing(moviereview_split)
```

```{r}
train %>%
  mutate(n_words = tokenizers::count_words(text)) %>%
  ggplot(aes(n_words)) +
  geom_bar() +
  labs(
    x = "Number of words per reviews",
    y = "Number of reviews"
  )
```

I am goin to try 25 words and include 48540 words in the vocabulary.

```{r}
library(textrecipes)

max_words <- 48540
max_length <- 25

movie_rec <- recipe(~ text, data = train) %>%
  step_tokenize(text) %>%
  step_tokenfilter(text, max_tokens = max_words) %>%
  step_sequence_onehot(text, sequence_length = max_length)
```

The function `step_sequence_onehot` transforms the tokens into a numeric
value that are more appropriate for modelling. It is similar to 
`step_tf()` but the difference lies in that it takes into the account
the order of the token. From the dim function we can see what dimensions
is the right to choose for the max length and max words.

```{r}
movie_prep <- prep(movie_rec)
mov_train <- bake(movie_prep, new_data = NULL, composition = "matrix")
dim(mov_train)
```

Our first deep learning model embeds these moviereview text in sequences of vectors, flattens them, and then trains a dense network layer to predict whether the review was positive or not.


```{r}
library(keras)

dense_model <- keras_model_sequential() %>% 
  layer_embedding(
    input_dim = max_words + 1,
    output_dim = 25,
    input_length = max_length
  ) %>% 
  layer_flatten() %>% 
   layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
dense_model
```

Okay this is something new so let me take a long breath and go though the
different step.

- We use the function `keras_model_sequential()` to initalize the model.
- The first layer `layer_embedding()` handles the preprocessed data.
  Here we create a tensor from our train data. 
- The next `layer_flatten()` takes our two dimensions and make it to one 
  dimension.
- Lastly, we have 2 densely connected layers. The last layer has a sigmoid     activation function to give us an output between 0 and 1, since we want to model a probability for a binary classification problem.

We stil need a optimizer and a loss function for our model before we can
fit to the train data. 

When the neural network finishes passing a batch of data through the network, it needs a way to use the difference between the predicted values and true values to update the network’s weights. The algorithm that determines those weights is known as the optimization algorithm. Many optimizers are available within Keras itself11; you can even create custom optimizers if what you need isn’t on the list. We will start by using the [Adam optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/), a good default optimizer for many problems.

When traning a NN (neural network) we must have something we want
to minimize which is our loss function. A loss function takes two values;
the true value and the preiced value and return a ratio of how close they
are. We are gonna use **binary cross entropy** because it does a good job
in dealing with probabilities. It measures the distance between the distributions.

For aplying this we use `complie()` that update the model with the
optimizer, loss function and a measurement metric (here I choose accuracy). 

```{r}
dense_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

Now we can fit the model to the traning data. Here se set epochs = 30 meaning it loops though the traning data 30 times. 

```{r}
dense_history <- dense_model %>%
  fit(
    x = mov_train,
    y = train$tag,
    batch_size = 512,
    epochs = 30,
    validation_split = 0.25,
    verbose = FALSE
  )
```

Then we can visualize the traning loop. 

```{r}
plot(dense_history)
```

Here it seems we only want to train our model on 6-7 epochs.

For our first deep learning model, we used the Keras defaults for creating a validation split and tracking metrics, but we can use tidymodels functions to be more specific about these model characteristics. Instead of using the validation_split argument to `fit()`, we can create our own validation set using tidymodels and use validation_data argument for fit(). We create our validation split from the training set.

```{r}
set.seed(234)
mov_val <- validation_split(train, strata = tag)
```

The moviereview-split object contains the information necessary to extract the data we will use for training/analysis and the data we will use for validation/assessment. We can extract these datasets in their raw, unprocessed form from the split using the helper functions analysis() and assessment(). Then, we can apply our prepped preprocessing recipe kick_prep to both to transform this data to the appropriate format for our neural network architecture.

```{r}
mov_analysis <- bake(movie_prep,
  new_data = analysis(mov_val$splits[[1]]),
  composition = "matrix"
)
dim(mov_analysis)
```

```{r}
mov_assess <- bake(movie_prep,
  new_data = assessment(mov_val$splits[[1]]),
  composition = "matrix"
)
dim(mov_assess)
```

```{r}
state_analysis <- analysis(mov_val$splits[[1]]) %>% pull(tag)
state_assess <- assessment(mov_val$splits[[1]]) %>% pull(tag)
```

Let’s set up our same dense neural network architecture.

```{r}
dense_model <- keras_model_sequential() %>%
  layer_embedding(
    input_dim = max_words + 1,
    output_dim = 25,
    input_length = max_length
  ) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

dense_model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```



Now we can fit this model with dropout to kick_analysis and validate on kick_assess. Let’s only fit for 7 epochs this time.

```{r}
val_history <- dense_model %>%
  fit(
    x = mov_analysis,
    y = state_analysis,
    batch_size = 512,
    epochs = 10,
    validation_data = list(mov_assess, state_assess),
    verbose = FALSE
  )

val_history
```

```{r}
plot(val_history)
```

Using our own validation set also allows us to flexibly measure performance using tidymodels function from the yardstick package. We do need to set up a few transformations between Keras and tidymodels. The following function keras_predict() creates a little bridge between the two frameworks, combining a Keras model with baked (i.e. preprocessed) data and returns the predictions in a tibble format.

```{r}
library(dplyr)

keras_predict <- function(model, baked_data, response) {
  predictions <- predict(model, baked_data)[, 1]

  tibble(
    .pred_1 = predictions,
    .pred_class = if_else(.pred_1 < 0.5, 0, 1),
    state = response
  ) %>%
    mutate(across(
      c(state, .pred_class), 
      ~ factor(.x, levels = c(1, 0))
    )) 
}
```

This function only works with binary classification models that take a preprocessed matrix as input and return a single probability for each observation. It returns both the predicted probability as well as the predicted class, using a 50% probability threshold.

This function creates prediction results that seamlessly connect with tidymodels and yardstick functions.



```{r}
val_res <- keras_predict(dense_model, mov_assess, state_assess)
val_res
```

```{r}
metrics(val_res, state, .pred_class)
```

The model certainly isn’t perfect; its accuracy is a little over 60%, but at least it is more or less evenly good at predicting both classes.

```{r}
val_res %>%
  roc_curve(truth = state, .pred_1) %>%
  autoplot() +
  labs(
    title = "Receiver operator curve for Movie reviews"
  )
```

So we want to go with the dense model and try to predict the test set.

```{r}
df_test <- bake(movie_prep,
  new_data = test,
  composition = "matrix"
)
final_res <- keras_predict(dense_model, df_test, test$tag)
final_res %>% metrics(state, .pred_class, .pred_1)
```

Here we get a accuracy on 60% which is not great at all and there is room 
for adjusting the model so it perform better.

## Conclusion

In this first post introrduction to deep learning we have build a model 
to predict movie reviews. It was compared to the performace for a Naive Bayes
model and both model did´t do the best job in classifiying the reviews.

The deep learning model was a simple first attemp to build a NNE framework.
This post has to be seen in a continuation of the other post that will come
where I wil explore furthere the different models and how they can be adjusted
to get better results. 


